{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f121f743",
   "metadata": {},
   "source": [
    "# Ablation Study 2.6: Data Augmentation Analysis\n",
    "\n",
    "## Motivation\n",
    "\n",
    "The current training pipeline uses three data augmentation techniques:\n",
    "1. Random point dropout\n",
    "2. Random scale (scaling point clouds)\n",
    "3. Random shift (translating point clouds)\n",
    "\n",
    "This ablation examines the contribution of each augmentation technique individually and in combination to understand which augmentations are most beneficial for model performance.\n",
    "\n",
    "## Experimental Plan\n",
    "\n",
    "1. Test different augmentation combinations:\n",
    "   - No augmentation (baseline)\n",
    "   - Only dropout\n",
    "   - Only scale\n",
    "   - Only shift\n",
    "   - Dropout + Scale\n",
    "   - Dropout + Shift\n",
    "   - Scale + Shift\n",
    "   - All augmentations (current default)\n",
    "2. Keep all other hyperparameters constant (step=1, temp=5.0, spike=True)\n",
    "3. Train models on ModelNet40 dataset\n",
    "4. Compare accuracy and generalization\n",
    "\n",
    "## Expected Insight\n",
    "\n",
    "This experiment reveals which augmentation techniques contribute most to model performance and generalization. Some augmentations may be redundant or even harmful, while others may be critical for good performance.\n",
    "\n",
    "## Dataset Setup\n",
    "\n",
    "Before running this notebook, ensure the ModelNet40 dataset is downloaded and extracted to: `data/modelnet40_normal_resampled/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354f8450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import datetime\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('..')\n",
    "from data_utils.ModelNetDataLoader import ModelNetDataLoader\n",
    "from models.spike_model import SpikeModel\n",
    "import provider\n",
    "\n",
    "print('Imports successful!')\n",
    "from cache_utils import load_training_history, save_training_history, cache_checkpoint, load_cached_checkpoint_path, best_metric\n",
    "from viz_utils import plot_training_curves, summarize_histories, plot_metric_table, plot_metric_bars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97973c94",
   "metadata": {},
   "source": [
    "## Augmentation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e488f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_augmentations(points, use_dropout=True, use_scale=True, use_shift=True):\n",
    "    \"\"\"Apply selected augmentations to point cloud data\"\"\"\n",
    "    if use_dropout:\n",
    "        points = provider.random_point_dropout(points)\n",
    "    if use_scale:\n",
    "        points[:,:,0:3] = provider.random_scale_point_cloud(points[:,:,0:3])\n",
    "    if use_shift:\n",
    "        points[:,:,0:3] = provider.shift_point_cloud(points[:,:,0:3])\n",
    "    return points\n",
    "\n",
    "print('Augmentation functions defined!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55412393",
   "metadata": {},
   "source": [
    "## Configuration for Different Augmentation Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ebf710",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, use_dropout=True, use_scale=True, use_shift=True, num_category=40):\n",
    "        self.use_cpu = False\n",
    "        self.gpu = '0'\n",
    "        self.batch_size = 24\n",
    "        self.model = 'pointnet_cls'\n",
    "        self.num_category = num_category\n",
    "        self.epoch = 200\n",
    "        self.learning_rate = 0.001\n",
    "        self.num_point = 1024\n",
    "        self.optimizer = 'Adam'\n",
    "        \n",
    "        # Create descriptive log directory name\n",
    "        aug_parts = []\n",
    "        if use_dropout:\n",
    "            aug_parts.append('dropout')\n",
    "        if use_scale:\n",
    "            aug_parts.append('scale')\n",
    "        if use_shift:\n",
    "            aug_parts.append('shift')\n",
    "        aug_str = '_'.join(aug_parts) if aug_parts else 'no_aug'\n",
    "        self.log_dir = f'ablation_{aug_str}_modelnet{num_category}'\n",
    "        \n",
    "        self.decay_rate = 1e-4\n",
    "        self.use_normals = False\n",
    "        self.process_data = False\n",
    "        self.use_uniform_sample = False\n",
    "        self.step = 1\n",
    "        self.spike = True\n",
    "        self.temp = 5.0\n",
    "        self.use_dropout = use_dropout\n",
    "        self.use_scale = use_scale\n",
    "        self.use_shift = use_shift\n",
    "\n",
    "# Create configurations for different augmentation combinations\n",
    "augmentation_configs = [\n",
    "    {'name': 'No Augmentation', 'dropout': False, 'scale': False, 'shift': False},\n",
    "    {'name': 'Only Dropout', 'dropout': True, 'scale': False, 'shift': False},\n",
    "    {'name': 'Only Scale', 'dropout': False, 'scale': True, 'shift': False},\n",
    "    {'name': 'Only Shift', 'dropout': False, 'scale': False, 'shift': True},\n",
    "    {'name': 'Dropout + Scale', 'dropout': True, 'scale': True, 'shift': False},\n",
    "    {'name': 'Dropout + Shift', 'dropout': True, 'scale': False, 'shift': True},\n",
    "    {'name': 'Scale + Shift', 'dropout': False, 'scale': True, 'shift': True},\n",
    "    {'name': 'All Augmentations', 'dropout': True, 'scale': True, 'shift': True},\n",
    "]\n",
    "\n",
    "args_list = [Args(use_dropout=cfg['dropout'], use_scale=cfg['scale'], use_shift=cfg['shift'], num_category=40) \n",
    "             for cfg in augmentation_configs]\n",
    "\n",
    "for cfg, args in zip(augmentation_configs, args_list):\n",
    "    print(f\"{cfg['name']}: {args.log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19236a4",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e3744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_experiment(args):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "    exp_dir = Path('../log/classification') / args.log_dir\n",
    "    exp_dir.mkdir(parents=True, exist_ok=True)\n",
    "    checkpoints_dir = exp_dir / 'checkpoints'\n",
    "    checkpoints_dir.mkdir(exist_ok=True)\n",
    "    return exp_dir, checkpoints_dir\n",
    "\n",
    "def load_data(args):\n",
    "    data_path = 'C:\\\\Users\\\\VIICTTE\\\\ML_Project\\\\modelnet40_normal_resampled'\n",
    "    train_dataset = ModelNetDataLoader(root=data_path, args=args, split='train')\n",
    "    test_dataset = ModelNetDataLoader(root=data_path, args=args, split='test')\n",
    "    trainDataLoader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "    testDataLoader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4)\n",
    "    return trainDataLoader, testDataLoader\n",
    "\n",
    "def create_model(args):\n",
    "    sys.path.append('../models')\n",
    "    model = importlib.import_module(args.model)\n",
    "    classifier = model.get_model(args.num_category, normal_channel=args.use_normals)\n",
    "    if args.spike:\n",
    "        classifier = SpikeModel(classifier, args.step, args.temp)\n",
    "        classifier.set_spike_state(True)\n",
    "    criterion = model.get_loss()\n",
    "    if not args.use_cpu:\n",
    "        classifier = classifier.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    return classifier, criterion\n",
    "\n",
    "print('Helper functions defined!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba9c07b",
   "metadata": {},
   "source": [
    "## Training Function with Configurable Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcaad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(args, exp_dir, checkpoints_dir, max_epochs=None):\n",
    "    if max_epochs:\n",
    "        args.epoch = max_epochs\n",
    "    trainDataLoader, testDataLoader = load_data(args)\n",
    "    classifier, criterion = create_model(args)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=args.learning_rate, weight_decay=args.decay_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.7)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    history = []\n",
    "    \n",
    "    aug_desc = []\n",
    "    if args.use_dropout:\n",
    "        aug_desc.append('dropout')\n",
    "    if args.use_scale:\n",
    "        aug_desc.append('scale')\n",
    "    if args.use_shift:\n",
    "        aug_desc.append('shift')\n",
    "    aug_str = ', '.join(aug_desc) if aug_desc else 'no augmentation'\n",
    "    print(f'Training with: {aug_str}')\n",
    "    \n",
    "    for epoch in range(args.epoch):\n",
    "        print(f'Epoch {epoch+1}/{args.epoch}')\n",
    "        classifier.train()\n",
    "        scheduler.step()\n",
    "        mean_correct = []\n",
    "        \n",
    "        for points, target in tqdm(trainDataLoader):\n",
    "            optimizer.zero_grad()\n",
    "            points = points.data.numpy()\n",
    "            \n",
    "            # Apply selected augmentations\n",
    "            points = apply_augmentations(points, args.use_dropout, args.use_scale, args.use_shift)\n",
    "            \n",
    "            points = torch.Tensor(points).transpose(2, 1)\n",
    "            if not args.use_cpu:\n",
    "                points, target = points.cuda(), target.cuda()\n",
    "            pred, trans_feat = classifier(points)\n",
    "            loss = criterion(pred, target.long(), trans_feat)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pred_choice = pred.data.max(1)[1]\n",
    "            correct = pred_choice.eq(target.long().data).cpu().sum()\n",
    "            mean_correct.append(correct.item() / float(points.size()[0]))\n",
    "        \n",
    "        train_acc = np.mean(mean_correct)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            classifier.eval()\n",
    "            test_correct = []\n",
    "            for points, target in testDataLoader:\n",
    "                if not args.use_cpu:\n",
    "                    points, target = points.cuda(), target.cuda()\n",
    "                points = points.transpose(2, 1)\n",
    "                pred, _ = classifier(points)\n",
    "                pred_choice = pred.data.max(1)[1]\n",
    "                correct = pred_choice.eq(target.long().data).cpu().sum()\n",
    "                test_correct.append(correct.item() / float(points.size()[0]))\n",
    "            test_acc = np.mean(test_correct)\n",
    "        \n",
    "        print(f'Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        history.append({'epoch': epoch+1, 'train_acc': train_acc, 'test_acc': test_acc})\n",
    "        \n",
    "        if test_acc >= best_acc:\n",
    "            best_acc = test_acc\n",
    "            torch.save({'model_state_dict': classifier.state_dict()}, str(checkpoints_dir / 'best_model.pth'))\n",
    "    \n",
    "    return classifier, history, best_acc\n",
    "\n",
    "print('Training function defined!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bbbea2",
   "metadata": {},
   "source": [
    "## Train Models with Different Augmentation Combinations\n",
    "\n",
    "**Note**: Training takes significant time. Consider reducing epochs for testing or testing fewer combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dcef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with different augmentation combinations\n",
    "BASELINE_CACHE_NAME = 'baseline_step_1'\n",
    "results = {}\n",
    "\n",
    "for cfg, args in zip(augmentation_configs, args_list):\n",
    "    print(f\"\n",
    "=== Training: {cfg['name']} ===\")\n",
    "    is_baseline = cfg['dropout'] and cfg['scale'] and cfg['shift']\n",
    "\n",
    "    if is_baseline:\n",
    "        baseline_history, baseline_meta = load_training_history(BASELINE_CACHE_NAME, with_metadata=True)\n",
    "        if baseline_history:\n",
    "            best_acc = best_metric(baseline_history, ['test_acc', 'test_instance_acc']) or 0.0\n",
    "            print(f\"Loaded cached baseline history with {len(baseline_history)} epoch(s). Best accuracy: {best_acc:.4f}\")\n",
    "            results[cfg['name']] = {\n",
    "                'classifier': None,\n",
    "                'history': baseline_history,\n",
    "                'best_acc': best_acc,\n",
    "                'config': cfg,\n",
    "                'metadata': baseline_meta,\n",
    "            }\n",
    "            cached_ckpt = load_cached_checkpoint_path(BASELINE_CACHE_NAME)\n",
    "            if cached_ckpt:\n",
    "                print(f'Cached baseline checkpoint available at: {cached_ckpt}')\n",
    "            continue\n",
    "        print('No cached baseline history found; training full-augmentation baseline from scratch.')\n",
    "\n",
    "    exp_dir, ckpt_dir = setup_experiment(args)\n",
    "    classifier, history, best_acc = train_model(args, exp_dir, ckpt_dir, max_epochs=10)  # Reduce to 10 for testing\n",
    "\n",
    "    if is_baseline:\n",
    "        metadata = {\n",
    "            'variant': 'baseline_all_augmentations',\n",
    "            'config': dict(vars(args)),\n",
    "            'max_epochs': args.epoch,\n",
    "            'augmentation_config': cfg,\n",
    "        }\n",
    "        history_path = save_training_history(history, BASELINE_CACHE_NAME, metadata=metadata)\n",
    "        print(f'Saved baseline history to {history_path}')\n",
    "        best_ckpt = ckpt_dir / 'best_model.pth'\n",
    "        if best_ckpt.exists():\n",
    "            cached_ckpt = cache_checkpoint(best_ckpt, BASELINE_CACHE_NAME)\n",
    "            print(f'Cached baseline checkpoint to {cached_ckpt}')\n",
    "\n",
    "    results[cfg['name']] = {\n",
    "        'classifier': classifier,\n",
    "        'history': history,\n",
    "        'best_acc': best_acc,\n",
    "        'config': cfg,\n",
    "    }\n",
    "    print(f\"{cfg['name']}: Best Accuracy = {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4967193b",
   "metadata": {},
   "source": [
    "## Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf40cfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "metrics = {\n",
    "    \"Train Accuracy\": [\"train_acc\"],\n",
    "    \"Test Accuracy\": [\"test_instance_acc\", \"test_acc\"],\n",
    "}\n",
    "\n",
    "available_histories = {}\n",
    "if 'results' in locals():\n",
    "    for cfg in augmentation_configs:\n",
    "        name = cfg['name']\n",
    "        entry = results.get(name, {})\n",
    "        history = entry.get('history', []) if isinstance(entry, dict) else []\n",
    "        if history:\n",
    "            available_histories[name] = history\n",
    "\n",
    "if not available_histories:\n",
    "    print('No training history available for visualization. Run the training cells above first.')\n",
    "else:\n",
    "    fig, axes = plot_training_curves(\n",
    "        available_histories,\n",
    "        metrics,\n",
    "        title=\"Augmentation Ablation: Accuracy per Epoch\",\n",
    "        max_cols=2\n",
    "    )\n",
    "\n",
    "    figures_dir = Path(\"../log/figures\") / \"augmentation\"\n",
    "    figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "    curve_path = figures_dir / \"accuracy_curves.png\"\n",
    "    fig.savefig(curve_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    summary_stats = summarize_histories(available_histories, metrics)\n",
    "    table_fig, table_ax = plot_metric_table(\n",
    "        summary_stats,\n",
    "        title=\"Augmentation Ablation Summary\",\n",
    "        value_fmt=\"{:.4f}\",\n",
    "        include_first=True\n",
    "    )\n",
    "    table_path = figures_dir / \"accuracy_summary.png\"\n",
    "    table_fig.savefig(table_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    bar_fig, bar_ax = plot_metric_bars(\n",
    "        summary_stats,\n",
    "        metric_name=\"Test Accuracy\",\n",
    "        title=\"Best Test Accuracy by Augmentation\",\n",
    "        ylabel=\"Test Accuracy\"\n",
    "    )\n",
    "    bar_path = figures_dir / \"best_accuracy.png\"\n",
    "    bar_fig.savefig(bar_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    baseline_best = summary_stats.get('No Augmentation', {}).get('Test Accuracy', {}).get('best')\n",
    "    if baseline_best is not None:\n",
    "        contrib_fig, contrib_ax = plt.subplots(figsize=(8, 4))\n",
    "        labels = []\n",
    "        deltas = []\n",
    "        for label, metric_stats in summary_stats.items():\n",
    "            best_value = metric_stats.get('Test Accuracy', {}).get('best')\n",
    "            if best_value is None:\n",
    "                continue\n",
    "            labels.append(label)\n",
    "            deltas.append(best_value - baseline_best)\n",
    "        bars = contrib_ax.bar(labels, deltas, color='#6a5acd')\n",
    "        contrib_ax.axhline(0.0, color='black', linestyle='--', linewidth=0.8)\n",
    "        contrib_ax.set_ylabel('Accuracy \u0394 vs. No Augmentation')\n",
    "        contrib_ax.set_title('Accuracy Gain Relative to Baseline Augmentation')\n",
    "        contrib_ax.grid(True, axis='y', linestyle='--', alpha=0.3)\n",
    "        for bar, delta in zip(bars, deltas):\n",
    "            contrib_ax.text(bar.get_x() + bar.get_width() / 2, delta, f'{delta*100:.2f}%', ha='center', va='bottom')\n",
    "        contrib_fig.tight_layout()\n",
    "        contrib_path = figures_dir / \"accuracy_delta.png\"\n",
    "        contrib_fig.savefig(contrib_path, dpi=150, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "    gap_fig, gap_ax = plt.subplots(figsize=(8, 4))\n",
    "    labels = []\n",
    "    gaps = []\n",
    "    for label, metric_stats in summary_stats.items():\n",
    "        train_last = metric_stats.get('Train Accuracy', {}).get('last')\n",
    "        test_last = metric_stats.get('Test Accuracy', {}).get('last')\n",
    "        if train_last is None or test_last is None:\n",
    "            continue\n",
    "        labels.append(label)\n",
    "        gaps.append(train_last - test_last)\n",
    "    if labels:\n",
    "        bars = gap_ax.bar(labels, gaps, color='#ff8c00')\n",
    "        gap_ax.axhline(0.0, color='black', linestyle='--', linewidth=0.8)\n",
    "        gap_ax.set_ylabel('Generalization Gap (Train - Test)')\n",
    "        gap_ax.set_title('Generalization Gap by Augmentation Policy')\n",
    "        gap_ax.grid(True, axis='y', linestyle='--', alpha=0.3)\n",
    "        for bar, gap in zip(bars, gaps):\n",
    "            gap_ax.text(bar.get_x() + bar.get_width() / 2, gap, f'{gap*100:.2f}%', ha='center', va='bottom')\n",
    "        gap_fig.tight_layout()\n",
    "        gap_path = figures_dir / \"generalization_gap.png\"\n",
    "        gap_fig.savefig(gap_path, dpi=150, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "    def _fmt(value):\n",
    "        return '-' if value is None else f\"{value:.4f}\"\n",
    "\n",
    "    print('\n",
    "Detailed metrics:')\n",
    "    for label, metric_stats in summary_stats.items():\n",
    "        train_stats = metric_stats.get('Train Accuracy', {})\n",
    "        test_stats = metric_stats.get('Test Accuracy', {})\n",
    "        print(\n",
    "            f\"  {label}: train_last={{_fmt(train_stats.get('last'))}}, test_last={{_fmt(test_stats.get('last'))}}, best_test={{_fmt(test_stats.get('best'))}}\"\n",
    "        )\n",
    "\n",
    "    if baseline_best is not None:\n",
    "        print(f\"\n",
    "Baseline (No Augmentation) best test accuracy: {baseline_best:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357bbd00",
   "metadata": {},
   "source": [
    "## Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9008f85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('ABLATION STUDY 2.6 SUMMARY: DATA AUGMENTATION')\n",
    "print('='*60)\n",
    "\n",
    "print('\\n1. Final Accuracy Results:')\n",
    "for cfg in augmentation_configs:\n",
    "    acc = results[cfg['name']]['best_acc']\n",
    "    print(f\"   {cfg['name']:20s}: {acc:.4f}\")\n",
    "\n",
    "best_config = max(augmentation_configs, key=lambda cfg: results[cfg['name']]['best_acc'])\n",
    "best_acc = results[best_config['name']]['best_acc']\n",
    "print(f\"\\n2. Best Configuration: {best_config['name']} with accuracy {best_acc:.4f}\")\n",
    "\n",
    "print('\\n3. Individual Augmentation Contributions:')\n",
    "baseline_acc = results['No Augmentation']['best_acc']\n",
    "for aug_name in ['Only Dropout', 'Only Scale', 'Only Shift']:\n",
    "    acc = results[aug_name]['best_acc']\n",
    "    improvement = (acc - baseline_acc) * 100\n",
    "    print(f\"   {aug_name:15s}: +{improvement:5.2f}% over baseline\")\n",
    "\n",
    "print('\\n4. Generalization Analysis:')\n",
    "for cfg in augmentation_configs:\n",
    "    history = results[cfg['name']]['history']\n",
    "    final_train = history[-1]['train_acc']\n",
    "    final_test = history[-1]['test_acc']\n",
    "    gap = (final_train - final_test) * 100\n",
    "    print(f\"   {cfg['name']:20s}: {gap:5.2f}% gap (train-test)\")\n",
    "\n",
    "print('\\n5. Key Insights:')\n",
    "if best_config['name'] == 'All Augmentations':\n",
    "    print('   - All augmentations together provide the best performance')\n",
    "    print('   - Augmentations are complementary and work well in combination')\n",
    "    print('   - Current default configuration is optimal')\n",
    "elif best_config['name'] == 'No Augmentation':\n",
    "    print('   - Augmentations do NOT help performance')\n",
    "    print('   - Model may be overfitting to augmented data')\n",
    "    print('   - Consider removing augmentations or adjusting their parameters')\n",
    "else:\n",
    "    print(f\"   - {best_config['name']} provides the best performance\")\n",
    "    print('   - Not all augmentations are beneficial')\n",
    "    print('   - Some augmentations may be redundant or harmful')\n",
    "\n",
    "print('\\n6. Recommendations:')\n",
    "most_important = max(['Only Dropout', 'Only Scale', 'Only Shift'], \n",
    "                     key=lambda name: results[name]['best_acc'])\n",
    "print(f\"   - Most important single augmentation: {most_important}\")\n",
    "print(f\"   - Best overall configuration: {best_config['name']}\")\n",
    "\n",
    "if results['All Augmentations']['best_acc'] < best_acc:\n",
    "    print('   - Consider using fewer augmentations for better performance')\n",
    "    print('   - Some augmentations may be introducing too much noise')\n",
    "else:\n",
    "    print('   - Keep all augmentations for robust training')\n",
    "    print('   - Augmentations improve generalization')\n",
    "\n",
    "print('\\n' + '='*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}